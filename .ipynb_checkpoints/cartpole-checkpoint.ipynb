{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbe3af0-5eaf-4458-89ec-d2ee2ee63be1",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning\n",
    "## Deep Reinforcement Learning\n",
    "\n",
    "&copy; Helena Aidos, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1deb331-76ae-4d4a-a39c-b5b84631fa15",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "Deep Reinforcement Learning (RL) can be computationally expensive, data-intensive, and prone to stability issues during training. However, advancements in algorithms, hardware, and techniques (e.g., distributed RL, transfer learning) continually improve its practicality and effectiveness. Thus, Deep RL is a powerful framework for solving problems where explicit programming or rule-based approaches fall short, making it a cornerstone of modern AI research and development.\n",
    "\n",
    "This notebook presents a framework for **Deep RL using JAX**, a library for array-oriented computation, with automatic differentiation and JIT compilation to enable high-performance machine learning research.\n",
    "\n",
    "To be able to run this notebook, make sure that you have installed the following packages:\n",
    "* jax (you probably installed it previously in TP6)\n",
    "* optax (you probably installed it previously in TP6)\n",
    "* gymnax\n",
    "* flax\n",
    "* distrax\n",
    "* chex\n",
    "\n",
    "If some of these packages are missing, you can install them the usual way. For instance, if you need to install `gymnax`, then\n",
    "```python\n",
    "!pip install gymnax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f05f2-5d45-4884-9fc5-e71fa696012f",
   "metadata": {},
   "source": [
    "### 2. Deep Q Network\n",
    "\n",
    "#### 2.1. Why do we need deep reinforcement learning?\n",
    "\n",
    "Simply put, Q-learning is an **off-policy** algorithm (the target policy is not the policy used for decision-making) that maintains and updates a Q-table, an explicit mapping of states to corresponding action values.\n",
    "\n",
    "While Q-learning is a practical solution for environments with discrete action spaces and restricted observation spaces, it struggles to scale well to more complex environments. Indeed, creating a Q-table requires **defining\n",
    "the action and observation spaces**.\n",
    "\n",
    "Consider the example of autonomous driving, the observation space is composed of an infinity of potential configurations derived from camera feeds and other sensory inputs. On the other hand, the action space\n",
    "includes a wide spectrum of steering wheel positions and varying levels of force applied to the brake and accelerator.\n",
    "\n",
    "Even though we could theoretically discretize the observation and action spaces, the sheer volume of possible states and actions leads to an **impractical Q-table** in real-world applications.\n",
    "\n",
    "Finding optimal actions in large and complex state-action spaces thus requires **powerful function approximation algorithms**, which is precisely what Neural Networks are. In the case of Deep Reinforcement Learning, neural nets are used as a replacement for the Q-table and provide an efficient solution to the curse of dimensionality introduced by large state spaces. Furthermore, we do not need to define the observation space explicitly.\n",
    "\n",
    "**Deep Reinforcement Learning (Deep RL) combines the strengths of deep learning and reinforcement learning**, enabling agents to learn complex behaviors and decision-making in environments with high-dimensional state and action spaces. Here's why Deep RL is valuable:\n",
    "\n",
    "1. **Handling High-Dimensional Inputs:** Traditional RL struggles with raw, unprocessed inputs like images or sensor data. Deep RL leverages deep neural networks to extract meaningful features from such inputs, making it effective for tasks like robotics, video games, and autonomous driving.\n",
    "2. **Learning Complex Policies:** Deep RL can learn sophisticated policies and strategies directly from data, even in environments with complex dynamics, sparse rewards, or long-term dependencies.\n",
    "3. **Generalization Across Tasks:** By using deep networks, agents can generalize better to unseen states or variations within the environment, allowing them to adapt to slightly different but related scenarios.\n",
    "4. **Scalability:** Deep RL scales well with modern computational resources and architectures, enabling the training of agents in expansive, continuous, or multi-agent environments.\n",
    "5. **Exploration and Adaptation:** Deep RL agents can explore their environment and adapt their behavior over time, making it suitable for dynamic and uncertain real-world problems.\n",
    "6. **End-to-End Learning:** Deep RL often requires minimal manual feature engineering, as neural networks learn the necessary representations directly from data, simplifying the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf50a98-a6bb-4dc3-bf39-af37f2201777",
   "metadata": {},
   "source": [
    "#### 2.2. Deep Q-networks (DQN)\n",
    "\n",
    "DQN uses two types of neural networks in parallel, starting with the \"online\" network, which is used for **Q-value prediction** and **decision-making**. On the other hand, the \"target\" network is used to **create stable Q-targets** to assess the performance of the online net via the loss function.\n",
    "\n",
    "Similarly to Q-learning, DQN agents are defined by two functions: `act` and `update`.\n",
    "\n",
    "##### 2.2.1. Act\n",
    "\n",
    "The `act` function implements an epsilon-greedy policy with respect to Q-values, which are estimated by the online neural network. In other words, the agent selects the action corresponding to the **maximum predicted Q-value** for a given state, with a set probability of acting randomly.\n",
    "\n",
    "You might remember that Q-learning updates its Q-table **after every step**. However, in Deep Learning, it is common practice to compute updates using **gradient descent on a batch of inputs**.\n",
    "\n",
    "For this reason, DQN stores experiences (tuples containing `state`, `action`, `reward`, `next_state`, `done_flag`) in a **replay buffer**. Instead of using only the last experience, we sample a batch of experiences from this buffer to train the network.\n",
    "\n",
    "<img src=\"DQN_action_selection.PNG\" alt='DQN_action_selection.PNG' width=\"500\"/>\n",
    "\n",
    "##### 2.2.2. Update\n",
    "\n",
    "The `update` function is responsible for training the network. It computes a **mean squared error** (MSE) loss based on the **temporal-difference** (TD) error:\n",
    "$$L(\\theta)=E\\left[ (r+(1-done) \\times \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta))^2 \\right]$$\n",
    "\n",
    "In this loss function, $\\theta$ denotes the **parameters of the online network**, and $\\theta^-$ represents the **parameters of the target network**. The parameters of the target network are set on the online network's parameters every $N$ steps, similar to a *checkpoint* ($N$ is a hyperparameter).\n",
    "\n",
    "This separation of parameters (with $\\theta$ for the current Q-values and $\\theta^-$ for the target (Q-values)) is crucial to stabilize training. Using the same parameters for both would be similar to aiming at a moving target, as updates to the network would immediately shift the target values. By **periodically updating** $\\theta^-$ (i.e., freezing these parameters for a set number of steps), we ensure **stable Q-targets** while the online network continues to learn.\n",
    "\n",
    "Finally, the $(1-done)$ term **adjusts the target for terminal states**. Indeed, when an episode ends (i.e., \"done\" is equal to 1), there is no next state. Therefore, the Q-value for the next state is set to 0.\n",
    "\n",
    "<img src=\"DQN_parameter_update.PNG\" alt='DQN_parameter_update.PNG' width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1ff1d-14a2-43a8-8bb2-56e89fc25317",
   "metadata": {},
   "source": [
    "#### 2.3. Replay buffers\n",
    "\n",
    "They are widely used in reinforcement learning for a variety of reasons:\n",
    "\n",
    "* **Generalization:** By sampling from the replay buffer, we break the correlation between consecutive experiences by mixing up their order. This way, we avoid overfitting to specific sequences of experiences.\n",
    "* **Diversity:** As the sampling is not limited to recent experiences, we generally observe a lower variance in updates and prevent overfitting to the latest experiences.\n",
    "* **Increased sample efficiency:** Each experience can be sampled multiple times from the buffer, enabling the model to learn more from individual experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf57ab-61e6-4677-b46f-26c90e7f3f89",
   "metadata": {},
   "source": [
    "### 3. Gymnax environments\n",
    "\n",
    "`gymnax` (documentation [here](https://pypi.org/project/gymnax/)) enhances the classic Gym API with the efficiency of JIT compilation and the scalability of vmap/pmap. It offers a diverse suite of environments, including classic control tasks, bsuite, MinAtar, and various classic/meta RL challenges. Gymnax provides precise functional control over environment settings, such as random seeds and hyperparameters, enabling faster and parallelized rollouts across different configurationsâ€”ideal for applications like meta-reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d357452-4999-491f-96fa-005bee5b5963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b4b6d-389a-4cb5-a40e-c01f201e4135",
   "metadata": {},
   "source": [
    "You can get an overview of all implemented environments as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f11f9-668c-40e4-a44b-8e38c0a61d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gymnax.registered_envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae898a7-399e-4e8a-b5dd-1907dca89446",
   "metadata": {},
   "source": [
    "#### 3.1. Cart Pole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa646032-03b3-4f17-9ec6-69af60ae7ba9",
   "metadata": {},
   "source": [
    "`gymnax` is similar to `gymnasium`. Let's take a look to the CartPole-v1 environment (documentation [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68475014-4e1b-4e3c-a658-fafe397bf240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_policy, key_step = jax.random.split(rng, 4)\n",
    "\n",
    "# create the CartPole-v1 environment\n",
    "env, env_params = gymnax.make('CartPole-v1')\n",
    "# inspect default environment settings\n",
    "env_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29cbc68-49ac-4bee-9c9e-91af6e1378d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation space dimensionality\n",
    "env.observation_space(env_params).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5d213-0504-482d-b77b-ef70611bed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action space dimensionality\n",
    "env.action_space(env_params).n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9392a-2fd7-4fd1-b81f-a05ec2813d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, action space dimensionality\n",
    "env.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefc78c-bf73-4fcd-8f91-13effb2357d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, state = env.reset(key_reset, env_params)\n",
    "obs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466adcc3-4750-4922-9345-38acc7368783",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space(env_params).sample(key_policy)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "n_obs, n_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a8cf4-3295-40ff-a6a4-d579dd6ea6a1",
   "metadata": {},
   "source": [
    "You can also simply use the environment with its default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c76bbd-81d4-4bbc-8688-43151d0c72d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, state = env.reset(key_reset)\n",
    "action = env.action_space().sample(key_policy)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502f10b-d7b3-41aa-b344-e9d50c67f178",
   "metadata": {},
   "source": [
    "`gymnax` provides fully functional environment dynamics that can leverage the full power of JAX's function transformations. E.g. one common RL use-case the parallel rollout of multiple workers. Using a `vmap` across random seeds (one per worker) allows us to implement such a parallelization on a single machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce96d46-ec44-4481-bbd6-6fd91ca60f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap_reset = jax.vmap(env.reset, in_axes=(0, None))\n",
    "vmap_step = jax.vmap(env.step, in_axes=(0, 0, 0, None))\n",
    "\n",
    "num_envs = 8\n",
    "vmap_keys = jax.random.split(rng, num_envs)\n",
    "\n",
    "obs, state = vmap_reset(vmap_keys, env_params)\n",
    "n_obs, n_state, reward, done, _ = vmap_step(vmap_keys, state, jax.numpy.zeros(num_envs), env_params)\n",
    "print(n_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de1c85-ebaf-4734-a9cb-3f3504ece06e",
   "metadata": {},
   "source": [
    "### 4. Reinforcement Learning with JAX\n",
    "\n",
    "`RLinJAX` is a library of deep reinforcement learning (RL) algorithms which you can `jax.jit` and `jax.vmap`. This notebook will show you how to set up, train and evaluate deep RL agents.\n",
    "\n",
    "The library has implemented several deep reinforcement learning algorithms, such as:\n",
    "* `dqn` - deep Q-network\n",
    "* `iqn` - implicit quantile network\n",
    "* `ppo` - proximal policy optimization\n",
    "* `pqn` - pointer Q-network\n",
    "* `sac` - soft actor-critic\n",
    "* `td3` - twin delayed DDPG (deep deterministic policy gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb13f0-2c96-483c-81d7-39a2da849278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from RLinJAX import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc72014-8016-4d54-88ed-84349c3ee1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ffa0e-1cda-41f4-ad71-7a12cb15cb3a",
   "metadata": {},
   "source": [
    "#### 4.1. Setting up the training configuration\n",
    "\n",
    "There are different types of parameters: environment, common, and algorithm-specific. These parameters can be setup using a dictionary.\n",
    "* **Environment parameters** are specific to the environment and include the maximum number of steps per episode, `max_steps_in_episode` (default: depends on the environment considered; 500 for the CartPole-v1 environment).\n",
    "* **Common parameters** are general parameters in common with all the implemented algorithms, and some are related to the training of the neural networks. The following are some of those parameters with the corresponding default values:\n",
    "    * `batch_size` = 256\n",
    "    * `learning_rate` = 0.0003\n",
    "    * `polyak` = 0.99 (see this [link](https://arxiv.org/html/2404.07525v1) for explanation)\n",
    "    * `num_epochs` = 1\n",
    "    * `num_envs` = 1\n",
    "    * `total_timesteps` = 131072 (corresponds to the number of episodes)\n",
    "    * `eval_freq` = 4096\n",
    "    * `target_update_freq` = 1\n",
    "    * `buffer_size` = 131072\n",
    "    * `fill_buffer` = 2048\n",
    "    * `gamma` = 0.99\n",
    "* **Algorithm-specific parameters** are specific to each algorithm (only presenting the two algorithms to be considered in the work):\n",
    "    * Parameters specific to algorithms using the epsilon-greedy method:\n",
    "        * `eps_start` = 1\n",
    "        * `eps_end` = 0.05\n",
    "        * `exploration_fraction` = 0.1 \n",
    "    * `dqn`:\n",
    "        * `agent` = \"QNetwork\" (another option is DuelingQNetwork)\n",
    "        * `activation` = \"swish\"\n",
    "        * `ddqn` = True (double deep q-network)\n",
    "    * `sac`:\n",
    "        * `activation` = \"relu\"\n",
    "        *  `target_entropy_ratio` = 0.98\n",
    "        *  `num_critics` = 2\n",
    "\n",
    "Let's see an example of how to set these parameters for the `dqn` agent in the `CartPole-v1` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8540d74-cec0-4bf7-8eef-75764203d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\"dqn\":{\"agent\": \"DuelingQNetwork\",\n",
    "                  \"agent_kwargs\": {\"activation\": \"swish\"},\n",
    "                  #\"num_envs\": 10,\n",
    "                  \"buffer_size\": 100_000,\n",
    "                  \"fill_buffer\": 1_000,\n",
    "                  \"batch_size\": 100,\n",
    "                  \"learning_rate\": 0.0003,\n",
    "                  \"polyak\": 0.98,\n",
    "                  \"num_epochs\": 5,\n",
    "                  \"target_update_freq\": 200,\n",
    "                  \"total_timesteps\": 100_000,\n",
    "                  \"eval_freq\": 5000,\n",
    "                  \"eps_start\": 1,\n",
    "                  \"eps_end\": 0.01,\n",
    "                  \"exploration_fraction\": 0.5,\n",
    "                  \"gamma\": 0.9,\n",
    "                  \"ddqn\": True\n",
    "                 }\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a822d10-0e5f-48d6-acd0-871c7fbd7b83",
   "metadata": {},
   "source": [
    "1. Each algorithm is represented as a class that extends `flax.PyTreeNode`\n",
    "2. `get_algo` is a convenient function to access algorithms easily\n",
    "3. `alg_cls.create` creates a frozen instance of the algorithm and populates it with default values\n",
    "\n",
    "Let's create the algorithm instance for DQN in the CartPole-v1 environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610ac77-5bc5-476d-9676-649713753488",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_str = \"dqn\"\n",
    "env_str = \"CartPole-v1\"\n",
    "\n",
    "algo_cls = get_algo(algo_str)\n",
    "algo = algo_cls.create(env=env_str, **CONFIGS.get(algo_str, {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb6c26-fabb-40f5-b529-06038acf9699",
   "metadata": {},
   "source": [
    "Let's look at the algorithm instance we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5243cb2-50d4-425c-bf27-9597ce259e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360664e8-4a7d-430a-966a-1c090277a59f",
   "metadata": {},
   "source": [
    "As you can see we have a lot of hyperparameters and variables we can tune. You are free to modify these after the creation, you just need to use the method `replace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacf3d4-3432-4f50-b99f-be3b4f131440",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = algo.replace(gamma=0.95)\n",
    "print(f\"New gamma: {algo.gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a81c6a-b330-43b9-b078-465f60f758e7",
   "metadata": {},
   "source": [
    "**A few words about configs**\n",
    "\n",
    "1. In `RLinJAX`, algorithms extend `flax.struct.PyTreeNode`(documentation [here](https://flax.readthedocs.io/en/latest/api_reference/flax.struct.html)). this allows to `jit` (documentation [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit)) and `vmap` (documentation [here](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap)) over individual parameters while keeping others fixed. For example, you can vmap over `learning_rate`, but not over `total_timesteps`.\n",
    "2. You are free to replace the config you pass to the training algorithm with any object that has the same (or necessary) attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7940dc-0e96-4c3f-a167-4e5c6bacc6cf",
   "metadata": {},
   "source": [
    "#### 4.2. Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff8860-ce4f-40ff-a3de-c2c6b390c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training seed and jit train function\n",
    "rng = jax.random.PRNGKey(0)\n",
    "train_fn = jax.jit(algo.train)\n",
    "\n",
    "print(\"Starting to train\")\n",
    "# Train\n",
    "start = time.time()\n",
    "train_state, evaluation = train_fn(rng)\n",
    "time_elapsed = time.time() - start\n",
    "\n",
    "sps = algo.total_timesteps / time_elapsed\n",
    "print(f\"Finished training in {time_elapsed:g} seconds ({sps:g} steps/second).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b586387-981e-4cca-9da1-e26dc6f807fc",
   "metadata": {},
   "source": [
    "Whoa, that was pretty quick! Let's break down what just happened.\n",
    "\n",
    "1. We jit `algo.train` to allow for fast execution\n",
    "2. `algo.train` created an initial `train_state`, which holds information about the algorithm's current state, such as the current environment step, replay buffer contents, network parameters, and more.\n",
    "3. The algorithm transforms the `train_state` during training. The final value of the `train_state`, including the final network parameters, is returned.\n",
    "4. Additionally, the returned `evaluation` value is a tuple of episode lengths and episodic returns. Each element of this tuple has the shape `(total_timesteps / eval_freq + 1, num_seeds)`. The `+1` comes from the fact that we also evaluate the initial policy per default. You can change the parameter `skip_initial_evaluation` in the initial CONFIG to skip the initial policy. Beware that smaller values of `eval_freq` will produce episodic return plots with much noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc668cc-3dda-4a19-9b06-a5e8c59ad716",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuple size:', len(evaluation))\n",
    "print('Episodic lengths and episodic returns shape:', evaluation[0].shape)\n",
    "print(f\"{algo.total_timesteps} / {algo.eval_freq} + 1 = {(algo.total_timesteps/algo.eval_freq+1)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b94430-3d46-4f64-bf32-ee66e039813a",
   "metadata": {},
   "source": [
    "**Note:** Depending on your computer, the next instruction can take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a1a9b-c55b-418b-a7e8-1d6c83957860",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171ae02-69f3-4c9e-afa6-bb7632545747",
   "metadata": {},
   "source": [
    "Let's look at how well training worked by plotting the learning curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d74cb3-e535-4c1b-88b1-36517c890b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "episode_lengths, episode_returns = evaluation\n",
    "mean_return = episode_returns.mean(axis=1)\n",
    "\n",
    "plt.plot(jax.numpy.linspace(0, algo.total_timesteps, len(mean_return)), mean_return)\n",
    "plt.xlabel(\"Environment step\")\n",
    "plt.ylabel(\"Episodic return\")\n",
    "plt.title(f\"Training agent for {env_str} using {algo_str}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef2c85-a89b-41e7-a120-f220cb3bdabc",
   "metadata": {},
   "source": [
    "We should also take a look at the `train_state`, to see what the algorithm produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c853a3c-baf4-460a-92ee-19d3f6321ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree_util.tree_map(lambda x: x.shape, train_state).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ed8c5-6ff6-4433-9a9e-ef942c36b466",
   "metadata": {},
   "source": [
    "In DQN algorithms, the `train_state` has a `q_ts` which is an instance of a `flax.training.train_state.TrainState` (documentation [here](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState)). This provides the network parameters of our final agent. Note that while also being a `flax.struct.PyTree`, our `train_state` is not. Algorithms which use buffers also have a `replay_buffer` state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63bea5-2310-4487-9990-5df5bec3ed15",
   "metadata": {},
   "source": [
    "#### 4.3. Making and evaluating policies\n",
    "\n",
    "As discussed above, algorithms return an agent's policy parameters in its train state. We can extract a policy of the type `Callable[[chex.Array, chex.PRNGKey], chex..Array]` which maps `(obs, rng) -> action` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1436b30-5863-4caa-8a15-7f7abbe931fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.make_act(train_state)\n",
    "policy = jax.jit(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3669f52-d0e7-41f0-b7d5-ad6751ee8c9e",
   "metadata": {},
   "source": [
    "Let's evaluate the policy! For demonstration purposes here is a full rollout like you would do with Gymnasium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b7b2a-493e-404a-8196-29fcc155e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "env, params = gymnax.make(env_str)\n",
    "step = jax.jit(env.step)\n",
    "\n",
    "obs, state = env.reset(rng, params)\n",
    "episode_return = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    rng, rng_action, rng_step = jax.random.split(rng, 3)\n",
    "    action = policy(obs, rng_action)\n",
    "    obs, state, reward, done, info = step(rng_step, state, action, params)\n",
    "    episode_return += reward\n",
    "\n",
    "print(f\"Return achieved in one episode of {env_str}: {episode_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b6770-429b-4117-b336-b788d20a3942",
   "metadata": {},
   "source": [
    "Alternatively, `RLinJAX` offers fast parallel evaluation of policies under the function `evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc7190-e81e-4091-8dd5-69956d03ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLinJAX.evaluate import evaluate\n",
    "\n",
    "num_seeds = 200  \n",
    "max_steps = params.max_steps_in_episode\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Evaluation time!\n",
    "episode_lengths, episode_returns = evaluate(policy, rng, env, params, num_seeds, max_steps)\n",
    "\n",
    "time_elapsed = time.time() - start\n",
    "\n",
    "print(\n",
    "    f\"Evaluated {num_seeds} episodes \"\n",
    "    f\"with a total of {jax.numpy.sum(episode_lengths)} environment steps \"\n",
    "    f\"in {time_elapsed:g} seconds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c546f5-a8b0-46f8-a985-f228c46c813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8, 4), ncols=2, sharey=\"row\")\n",
    "\n",
    "axes[0].hist(episode_lengths, bins=10)\n",
    "axes[0].set(title=\"Episode length\", ylabel=\"Count\")\n",
    "axes[1].hist(episode_returns)\n",
    "axes[1].set(title=\"Episode return\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0e178-aab4-40e0-926f-fc2adc5c529b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
