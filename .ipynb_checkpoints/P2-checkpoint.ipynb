{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc89fea5-ccac-4b1f-ba1c-57d6a0a7a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnax\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e0035-5a8e-46b4-bfb5-fd9a7db33759",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80c15fe5-fe71-497a-bd2f-c42f34b204fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_policy, key_step = jax.random.split(rng, 4)\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bee5a8e7-9e98-4867-8939-37333c46469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space low values: [-1.2  -0.07]\n",
      "Observation space high values: [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "# Get the observation space\n",
    "observation_space = env.observation_space\n",
    "# Print the min and max values for the observation space\n",
    "print(\"Observation space low values:\", observation_space.low)\n",
    "print(\"Observation space high values:\", observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4e45eb-8032-43d3-b014-4965a471d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space\n",
    "print(\"Number of actions:\", action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d563cc0-f8ab-45ea-8711-749305ab4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(action_space)\n",
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d5d17a0-05b1-49c5-a66a-7692b313e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "position_bins = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num_bins)\n",
    "velocity_bins = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num_bins)\n",
    "\n",
    "#print(position_bins)\n",
    "#print(velocity_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a840244-e28e-4730-9859-7c2c94a73f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table initialization\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros((num_bins, num_bins, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be6e0980-5378-42ca-b2ce-444929e4dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    position, velocity = state\n",
    "    position_bin = np.digitize(position, position_bins) - 1  \n",
    "    velocity_bin = np.digitize(velocity, velocity_bins) - 1\n",
    "    position_bin = np.clip(position_bin, 0, num_bins - 1)\n",
    "    velocity_bin = np.clip(velocity_bin, 0, num_bins - 1)\n",
    "    return position_bin, velocity_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3020528-298b-4ce8-8bd8-5099352e2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(num_actions))\n",
    "    else:\n",
    "        position_bin, velocity_bin = state\n",
    "        return np.argmax(q_values_table[position_bin, velocity_bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85cea76f-0eb7-471e-87d1-480a95ec95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, num_bins, num_episodes, num_steps, alpha, gamma, epsilon, epsilon_decay, min_epsilon):\n",
    "    # Initialize Q-table\n",
    "    num_actions = env.action_space.n\n",
    "    q_values_table = np.zeros((num_bins, num_bins, num_actions))\n",
    "    \n",
    "    # Tracking performance\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # Initialize the state by resetting the environment\n",
    "        state, _ = env.reset()\n",
    "    \n",
    "        # Discretize the state\n",
    "        discretized_state = discretize_state(state)\n",
    "    \n",
    "        total_reward = 0\n",
    "        for t in range(num_steps):\n",
    "            # Select the action using the epsilon-greedy policy\n",
    "            action = epsilon_greedy(discretized_state, epsilon)\n",
    "    \n",
    "            # Perform the selected action and store the next state information\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "            # Discretize the next state\n",
    "            discretized_next_state = discretize_state(next_state)\n",
    "    \n",
    "            # Find the action a' with the maximum Q-value in the next state\n",
    "            next_action = np.argmax(q_values_table[discretized_next_state])\n",
    "    \n",
    "            # Update Q-value of the state-action pair using the Q-learning update rule\n",
    "            q_values_table[discretized_state[0], discretized_state[1], action] += alpha * (\n",
    "                reward + gamma * q_values_table[discretized_next_state[0], discretized_next_state[1], next_action] - q_values_table[discretized_state[0], discretized_state[1], action]\n",
    "            )\n",
    "    \n",
    "            # Update current state to next state\n",
    "            discretized_state = discretized_next_state\n",
    "    \n",
    "            # Track total reward for the episode\n",
    "            total_reward += reward\n",
    "    \n",
    "            # If the current state is the terminal state, break\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        # Track performance\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(t)\n",
    "\n",
    "        #if (i + 1) % 100 == 0:\n",
    "        #    print(f\"Episode {i + 1}: Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return episode_rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2155f0a4-bdbd-46ab-ab65-efa997db8275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Best Hyperparameters: (0.13333875294963685, 0.8609157583736841, 0.9992401308627211, 0.07413601675463205)\n",
      "Best Average Reward: -311.87\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define ranges for hyperparameters\n",
    "alpha_range = (0.01, 0.15)\n",
    "gamma_range = (0.8, 0.99)\n",
    "epsilon_decay_range = (0.999, 0.9999)\n",
    "min_epsilon_range = (0.05, 0.1)\n",
    "\n",
    "num_trials = 30  # Number of random samples to test\n",
    "all_results = []\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample hyperparameters\n",
    "    alpha = random.uniform(*alpha_range)\n",
    "    gamma = random.uniform(*gamma_range)\n",
    "    epsilon_decay = random.uniform(*epsilon_decay_range)\n",
    "    min_epsilon = random.uniform(*min_epsilon_range)\n",
    "\n",
    "    # Train with sampled hyperparameters\n",
    "    episode_rewards, episode_lengths = train_q_learning(\n",
    "        env, num_bins, num_episodes=5000, num_steps=400,  # Reduce episodes for faster testing\n",
    "        alpha=alpha, gamma=gamma, epsilon=1.0,\n",
    "        epsilon_decay=epsilon_decay, min_epsilon=min_epsilon\n",
    "    )\n",
    "\n",
    "    # Record the results\n",
    "    avg_reward = sum(episode_rewards[-100:]) / 100  # Average reward over last 100 episodes\n",
    "    all_results.append((avg_reward, alpha, gamma, epsilon_decay, min_epsilon))\n",
    "    print(\"Done\")\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(all_results, key=lambda x: x[0])\n",
    "print(\"Best Hyperparameters:\", best_result[1:])\n",
    "print(\"Best Average Reward:\", best_result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bb4cd-55d0-4b36-83d8-0396a33a543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "054dceff-ce5c-452e-998e-adca79e25843",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon_decay \u001b[38;5;129;01min\u001b[39;00m epsilon_decay_values:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m min_epsilon \u001b[38;5;129;01min\u001b[39;00m min_epsilon_values:\n\u001b[0;32m---> 18\u001b[0m         episode_reward_list, episode_lengths_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         all_episode_reward_list\u001b[38;5;241m.\u001b[39mappend(episode_reward_list)\n\u001b[1;32m     20\u001b[0m         all_episode_lengths_list\u001b[38;5;241m.\u001b[39mappend(episode_lengths_list)\n",
      "Cell \u001b[0;32mIn[46], line 23\u001b[0m, in \u001b[0;36mtrain_q_learning\u001b[0;34m(env, num_bins, num_episodes, num_steps, alpha, gamma, epsilon, epsilon_decay, min_epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m epsilon_greedy(discretized_state, epsilon)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform the selected action and store the next state information\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Discretize the next state\u001b[39;00m\n\u001b[1;32m     26\u001b[0m discretized_next_state \u001b[38;5;241m=\u001b[39m discretize_state(next_state)\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/envs/classic_control/mountain_car.py:140\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    138\u001b[0m position, velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    139\u001b[0m velocity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m position) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgravity)\n\u001b[0;32m--> 140\u001b[0m velocity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvelocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m position \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m velocity\n\u001b[1;32m    142\u001b[0m position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_position)\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2247\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \n\u001b[1;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/numpy/_core/_methods.py:99\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         items \u001b[38;5;241m=\u001b[39m umr_sum(broadcast_to(where, arr\u001b[38;5;241m.\u001b[39mshape), axis, nt\u001b[38;5;241m.\u001b[39mintp, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m                         keepdims)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m items\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip\u001b[39m(a, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of max or min must be given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Q-Learning Algorithm\n",
    "num_episodes = 10000\n",
    "num_steps = 400  # Max steps per episode\n",
    "epsilon = 1.0  # Initial epsilon (exploration probability)\n",
    "\n",
    "alpha_values = [0.01, 0.1, 0.15]  # Learning rate\n",
    "gamma_values = [0.8, 0.9, 0.99]   # Discount factor\n",
    "epsilon_decay_values = [0.999, 0.9995, 0.9999]  # Epsilon decay\n",
    "min_epsilon_values = [0.05, 0.1] \n",
    "\n",
    "all_episode_reward_list = []\n",
    "all_episode_lengths_list = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for gamma in gamma_values:\n",
    "        for epsilon_decay in epsilon_decay_values:\n",
    "            for min_epsilon in min_epsilon_values:\n",
    "                episode_reward_list, episode_lengths_list = train_q_learning(env,num_bins,num_episodes,num_steps,alpha,gamma,epsilon,epsilon_decay,min_epsilon)\n",
    "                all_episode_reward_list.append(episode_reward_list)\n",
    "                all_episode_lengths_list.append(episode_lengths_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ebddf-09ec-4cbe-b379-538327a5202e",
   "metadata": {},
   "source": [
    "# Deep Q-Network (dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45889fe0-5143-4cca-8f1a-642ca89a6f8e",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ff5e5-b9c3-4fca-9491-ed4b22eb72fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
