{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc89fea5-ccac-4b1f-ba1c-57d6a0a7a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnax\n",
    "import gymnasium as gym\n",
    "import jax\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80c15fe5-fe71-497a-bd2f-c42f34b204fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_policy, key_step = jax.random.split(rng, 4)\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bee5a8e7-9e98-4867-8939-37333c46469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space low values: [-1.2  -0.07]\n",
      "Observation space high values: [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "# Get the observation space\n",
    "observation_space = env.observation_space\n",
    "# Print the min and max values for the observation space\n",
    "print(\"Observation space low values:\", observation_space.low)\n",
    "print(\"Observation space high values:\", observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da4e45eb-8032-43d3-b014-4965a471d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space\n",
    "print(\"Number of actions:\", action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d563cc0-f8ab-45ea-8711-749305ab4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(3)\n",
      "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(action_space)\n",
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d5d17a0-05b1-49c5-a66a-7692b313e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "position_bins = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num_bins)\n",
    "velocity_bins = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num_bins)\n",
    "\n",
    "#print(position_bins)\n",
    "#print(velocity_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a840244-e28e-4730-9859-7c2c94a73f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table initialization\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros((num_bins, num_bins, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be6e0980-5378-42ca-b2ce-444929e4dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    position, velocity = state\n",
    "    position_bin = np.digitize(position, position_bins) - 1  \n",
    "    velocity_bin = np.digitize(velocity, velocity_bins) - 1\n",
    "    position_bin = np.clip(position_bin, 0, num_bins - 1)\n",
    "    velocity_bin = np.clip(velocity_bin, 0, num_bins - 1)\n",
    "    return position_bin, velocity_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3020528-298b-4ce8-8bd8-5099352e2609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(num_actions))\n",
    "    else:\n",
    "        position_bin, velocity_bin = state\n",
    "        return np.argmax(q_values_table[position_bin, velocity_bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85cea76f-0eb7-471e-87d1-480a95ec95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, num_bins, num_episodes, num_steps, alpha, gamma, epsilon, epsilon_decay, min_epsilon):\n",
    "    # Initialize Q-table\n",
    "    num_actions = env.action_space.n\n",
    "    q_values_table = np.zeros((num_bins, num_bins, num_actions))\n",
    "    \n",
    "    # Tracking performance\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # Initialize the state by resetting the environment\n",
    "        state, _ = env.reset()\n",
    "    \n",
    "        # Discretize the state\n",
    "        discretized_state = discretize_state(state)\n",
    "    \n",
    "        total_reward = 0\n",
    "        for t in range(num_steps):\n",
    "            # Select the action using the epsilon-greedy policy\n",
    "            action = epsilon_greedy(discretized_state, epsilon)\n",
    "    \n",
    "            # Perform the selected action and store the next state information\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "            # Discretize the next state\n",
    "            discretized_next_state = discretize_state(next_state)\n",
    "    \n",
    "            # Find the action a' with the maximum Q-value in the next state\n",
    "            next_action = np.argmax(q_values_table[discretized_next_state])\n",
    "    \n",
    "            # Update Q-value of the state-action pair using the Q-learning update rule\n",
    "            q_values_table[discretized_state[0], discretized_state[1], action] += alpha * (\n",
    "                reward + gamma * q_values_table[discretized_next_state[0], discretized_next_state[1], next_action] - q_values_table[discretized_state[0], discretized_state[1], action]\n",
    "            )\n",
    "    \n",
    "            # Update current state to next state\n",
    "            discretized_state = discretized_next_state\n",
    "    \n",
    "            # Track total reward for the episode\n",
    "            total_reward += reward\n",
    "    \n",
    "            # If the current state is the terminal state, break\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "        # Track performance\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(t)\n",
    "\n",
    "        #if (i + 1) % 100 == 0:\n",
    "        #    print(f\"Episode {i + 1}: Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return episode_rewards, episode_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155f0a4-bdbd-46ab-ab65-efa997db8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define ranges for hyperparameters\n",
    "alpha_range = (0.01, 0.15)\n",
    "gamma_range = (0.8, 0.99)\n",
    "epsilon_decay_range = (0.999, 0.9999)\n",
    "min_epsilon_range = (0.05, 0.1)\n",
    "\n",
    "num_trials = 30  # Number of random samples to test\n",
    "all_results = []\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    # Randomly sample hyperparameters\n",
    "    alpha = random.uniform(*alpha_range)\n",
    "    gamma = random.uniform(*gamma_range)\n",
    "    epsilon_decay = random.uniform(*epsilon_decay_range)\n",
    "    min_epsilon = random.uniform(*min_epsilon_range)\n",
    "\n",
    "    # Train with sampled hyperparameters\n",
    "    episode_rewards, episode_lengths = train_q_learning(\n",
    "        env, num_bins, num_episodes=5000, num_steps=400,  # Reduce episodes for faster testing\n",
    "        alpha=alpha, gamma=gamma, epsilon=1.0,\n",
    "        epsilon_decay=epsilon_decay, min_epsilon=min_epsilon\n",
    "    )\n",
    "\n",
    "    # Record the results\n",
    "    avg_reward = sum(episode_rewards[-100:]) / 100  # Average reward over last 100 episodes\n",
    "    all_results.append((avg_reward, alpha, gamma, epsilon_decay, min_epsilon))\n",
    "    print(\"Done\")\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = max(all_results, key=lambda x: x[0])\n",
    "print(\"Best Hyperparameters:\", best_result[1:])\n",
    "print(\"Best Average Reward:\", best_result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "054dceff-ce5c-452e-998e-adca79e25843",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon_decay \u001b[38;5;129;01min\u001b[39;00m epsilon_decay_values:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m min_epsilon \u001b[38;5;129;01min\u001b[39;00m min_epsilon_values:\n\u001b[0;32m---> 18\u001b[0m         episode_reward_list, episode_lengths_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         all_episode_reward_list\u001b[38;5;241m.\u001b[39mappend(episode_reward_list)\n\u001b[1;32m     20\u001b[0m         all_episode_lengths_list\u001b[38;5;241m.\u001b[39mappend(episode_lengths_list)\n",
      "Cell \u001b[0;32mIn[46], line 23\u001b[0m, in \u001b[0;36mtrain_q_learning\u001b[0;34m(env, num_bins, num_episodes, num_steps, alpha, gamma, epsilon, epsilon_decay, min_epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m epsilon_greedy(discretized_state, epsilon)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform the selected action and store the next state information\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Discretize the next state\u001b[39;00m\n\u001b[1;32m     26\u001b[0m discretized_next_state \u001b[38;5;241m=\u001b[39m discretize_state(next_state)\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/gymnasium/envs/classic_control/mountain_car.py:140\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    138\u001b[0m position, velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    139\u001b[0m velocity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m position) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgravity)\n\u001b[0;32m--> 140\u001b[0m velocity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvelocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m position \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m velocity\n\u001b[1;32m    142\u001b[0m position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_position)\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2247\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2245\u001b[0m \n\u001b[1;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Documents/jupyter-workspace/AAA/lib/python3.12/site-packages/numpy/_core/_methods.py:99\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         items \u001b[38;5;241m=\u001b[39m umr_sum(broadcast_to(where, arr\u001b[38;5;241m.\u001b[39mshape), axis, nt\u001b[38;5;241m.\u001b[39mintp, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m                         keepdims)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m items\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip\u001b[39m(a, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmin\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of max or min must be given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Q-Learning Algorithm\n",
    "num_episodes = 10000\n",
    "num_steps = 400  # Max steps per episode\n",
    "epsilon = 1.0  # Initial epsilon (exploration probability)\n",
    "\n",
    "alpha_values = [0.01, 0.1, 0.15]  # Learning rate\n",
    "gamma_values = [0.8, 0.9, 0.99]   # Discount factor\n",
    "epsilon_decay_values = [0.999, 0.9995, 0.9999]  # Epsilon decay\n",
    "min_epsilon_values = [0.05, 0.1] \n",
    "\n",
    "all_episode_reward_list = []\n",
    "all_episode_lengths_list = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    for gamma in gamma_values:\n",
    "        for epsilon_decay in epsilon_decay_values:\n",
    "            for min_epsilon in min_epsilon_values:\n",
    "                episode_reward_list, episode_lengths_list = train_q_learning(env,num_bins,num_episodes,num_steps,alpha,gamma,epsilon,epsilon_decay,min_epsilon)\n",
    "                all_episode_reward_list.append(episode_reward_list)\n",
    "                all_episode_lengths_list.append(episode_lengths_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999a4ed-a4ad-405f-b802-e03fd264249b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee2f28-cfff-4df3-8abb-f9fbcc7f0f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4783c-288e-4f7a-a3fb-97eddff20485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5557b72-ab63-46e0-a598-83d0dd7fbf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a3b41-b5dd-405b-8e91-9b6b7657f04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f67399-b3bd-4535-952d-004eea96d65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebdbba1-0098-4e69-b336-766540f60f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6edde1d-8ab0-45c5-9580-2d7f44bdf3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79287d5f-33d1-4e33-b3ec-6fe8f44da32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccd86e-efc8-4743-8b2c-b4dc4e9c405c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43dfeb71-d96e-43d1-b311-8e1748eed423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Total Reward: -400.0, Epsilon: 0.95\n",
      "Episode 200: Total Reward: -400.0, Epsilon: 0.90\n",
      "Episode 300: Total Reward: -400.0, Epsilon: 0.86\n",
      "Episode 400: Total Reward: -400.0, Epsilon: 0.82\n",
      "Episode 500: Total Reward: -400.0, Epsilon: 0.78\n",
      "Episode 600: Total Reward: -400.0, Epsilon: 0.74\n",
      "Episode 700: Total Reward: -400.0, Epsilon: 0.70\n",
      "Episode 800: Total Reward: -400.0, Epsilon: 0.67\n",
      "Episode 900: Total Reward: -400.0, Epsilon: 0.64\n",
      "Episode 1000: Total Reward: -400.0, Epsilon: 0.61\n",
      "Episode 1100: Total Reward: -400.0, Epsilon: 0.58\n",
      "Episode 1200: Total Reward: -400.0, Epsilon: 0.55\n",
      "Episode 1300: Total Reward: -400.0, Epsilon: 0.52\n",
      "Episode 1400: Total Reward: -400.0, Epsilon: 0.50\n",
      "Episode 1500: Total Reward: -400.0, Epsilon: 0.47\n",
      "Episode 1600: Total Reward: -400.0, Epsilon: 0.45\n",
      "Episode 1700: Total Reward: -400.0, Epsilon: 0.43\n",
      "Episode 1800: Total Reward: -400.0, Epsilon: 0.41\n",
      "Episode 1900: Total Reward: -400.0, Epsilon: 0.39\n",
      "Episode 2000: Total Reward: -400.0, Epsilon: 0.37\n",
      "Episode 2100: Total Reward: -400.0, Epsilon: 0.35\n",
      "Episode 2200: Total Reward: -400.0, Epsilon: 0.33\n",
      "Episode 2300: Total Reward: -400.0, Epsilon: 0.32\n",
      "Episode 2400: Total Reward: -400.0, Epsilon: 0.30\n",
      "Episode 2500: Total Reward: -400.0, Epsilon: 0.29\n",
      "Episode 2600: Total Reward: -400.0, Epsilon: 0.27\n",
      "Episode 2700: Total Reward: -400.0, Epsilon: 0.26\n",
      "Episode 2800: Total Reward: -358.0, Epsilon: 0.25\n",
      "Episode 2900: Total Reward: -400.0, Epsilon: 0.23\n",
      "Episode 3000: Total Reward: -400.0, Epsilon: 0.22\n",
      "Episode 3100: Total Reward: -400.0, Epsilon: 0.21\n",
      "Episode 3200: Total Reward: -400.0, Epsilon: 0.20\n",
      "Episode 3300: Total Reward: -400.0, Epsilon: 0.19\n",
      "Episode 3400: Total Reward: -308.0, Epsilon: 0.18\n",
      "Episode 3500: Total Reward: -337.0, Epsilon: 0.17\n",
      "Episode 3600: Total Reward: -400.0, Epsilon: 0.17\n",
      "Episode 3700: Total Reward: -400.0, Epsilon: 0.16\n",
      "Episode 3800: Total Reward: -400.0, Epsilon: 0.15\n",
      "Episode 3900: Total Reward: -400.0, Epsilon: 0.14\n",
      "Episode 4000: Total Reward: -400.0, Epsilon: 0.14\n",
      "Episode 4100: Total Reward: -316.0, Epsilon: 0.13\n",
      "Episode 4200: Total Reward: -387.0, Epsilon: 0.12\n",
      "Episode 4300: Total Reward: -400.0, Epsilon: 0.12\n",
      "Episode 4400: Total Reward: -400.0, Epsilon: 0.11\n",
      "Episode 4500: Total Reward: -400.0, Epsilon: 0.11\n",
      "Episode 4600: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 4700: Total Reward: -353.0, Epsilon: 0.10\n",
      "Episode 4800: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 4900: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 5000: Total Reward: -276.0, Epsilon: 0.10\n",
      "Episode 5100: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 5200: Total Reward: -341.0, Epsilon: 0.10\n",
      "Episode 5300: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 5400: Total Reward: -308.0, Epsilon: 0.10\n",
      "Episode 5500: Total Reward: -395.0, Epsilon: 0.10\n",
      "Episode 5600: Total Reward: -369.0, Epsilon: 0.10\n",
      "Episode 5700: Total Reward: -329.0, Epsilon: 0.10\n",
      "Episode 5800: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 5900: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 6000: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 6100: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 6200: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 6300: Total Reward: -322.0, Epsilon: 0.10\n",
      "Episode 6400: Total Reward: -328.0, Epsilon: 0.10\n",
      "Episode 6500: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 6600: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 6700: Total Reward: -330.0, Epsilon: 0.10\n",
      "Episode 6800: Total Reward: -372.0, Epsilon: 0.10\n",
      "Episode 6900: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 7000: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 7100: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 7200: Total Reward: -320.0, Epsilon: 0.10\n",
      "Episode 7300: Total Reward: -285.0, Epsilon: 0.10\n",
      "Episode 7400: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 7500: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 7600: Total Reward: -388.0, Epsilon: 0.10\n",
      "Episode 7700: Total Reward: -367.0, Epsilon: 0.10\n",
      "Episode 7800: Total Reward: -298.0, Epsilon: 0.10\n",
      "Episode 7900: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 8000: Total Reward: -359.0, Epsilon: 0.10\n",
      "Episode 8100: Total Reward: -374.0, Epsilon: 0.10\n",
      "Episode 8200: Total Reward: -323.0, Epsilon: 0.10\n",
      "Episode 8300: Total Reward: -327.0, Epsilon: 0.10\n",
      "Episode 8400: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 8500: Total Reward: -242.0, Epsilon: 0.10\n",
      "Episode 8600: Total Reward: -239.0, Epsilon: 0.10\n",
      "Episode 8700: Total Reward: -327.0, Epsilon: 0.10\n",
      "Episode 8800: Total Reward: -241.0, Epsilon: 0.10\n",
      "Episode 8900: Total Reward: -373.0, Epsilon: 0.10\n",
      "Episode 9000: Total Reward: -307.0, Epsilon: 0.10\n",
      "Episode 9100: Total Reward: -338.0, Epsilon: 0.10\n",
      "Episode 9200: Total Reward: -270.0, Epsilon: 0.10\n",
      "Episode 9300: Total Reward: -291.0, Epsilon: 0.10\n",
      "Episode 9400: Total Reward: -229.0, Epsilon: 0.10\n",
      "Episode 9500: Total Reward: -400.0, Epsilon: 0.10\n",
      "Episode 9600: Total Reward: -237.0, Epsilon: 0.10\n",
      "Episode 9700: Total Reward: -320.0, Epsilon: 0.10\n",
      "Episode 9800: Total Reward: -382.0, Epsilon: 0.10\n",
      "Episode 9900: Total Reward: -372.0, Epsilon: 0.10\n",
      "Episode 10000: Total Reward: -252.0, Epsilon: 0.10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Plotting the results\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Plot total reward per episode\u001b[39;00m\n\u001b[1;32m     67\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "q_values_table = np.zeros((num_bins, num_bins, num_actions))\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "num_episodes = 10000\n",
    "num_steps = 400  # Max steps per episode\n",
    "alpha = 0.11  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Initial epsilon (exploration probability)\n",
    "epsilon_decay = 0.9995  # epsilon decay\n",
    "min_epsilon = 0.1  # instead of 0.01\n",
    "\n",
    "# Track performance\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Initialize the state by resetting the environment\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    # Discretize the state\n",
    "    discretized_state = discretize_state(state)\n",
    "\n",
    "    total_reward = 0\n",
    "    for t in range(num_steps):\n",
    "        # Select the action using the epsilon-greedy policy\n",
    "        action = epsilon_greedy(discretized_state, epsilon)\n",
    "\n",
    "        # Perform the selected action and store the next state information\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Discretize the next state\n",
    "        discretized_next_state = discretize_state(next_state)\n",
    "\n",
    "        # Find the action a' with the maximum Q-value in the next state\n",
    "        next_action = np.argmax(q_values_table[discretized_next_state])\n",
    "\n",
    "        # Update Q-value of the state-action pair using the Q-learning update rule\n",
    "        q_values_table[discretized_state[0], discretized_state[1], action] += alpha * (\n",
    "            reward + gamma * q_values_table[discretized_next_state[0], discretized_next_state[1], next_action] - q_values_table[discretized_state[0], discretized_state[1], action]\n",
    "        )\n",
    "\n",
    "        # Update current state to next state\n",
    "        discretized_state = discretized_next_state\n",
    "\n",
    "        # Track total reward for the episode\n",
    "        total_reward += reward\n",
    "\n",
    "        # If the current state is the terminal state, break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon to reduce exploration over time\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Record episode performance\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(t)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Episode {i + 1}: Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot total reward per episode\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "# Plot episode lengths\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(episode_lengths)\n",
    "plt.title('Steps per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa655ad-8b11-4dca-9d2a-374301b21bfe",
   "metadata": {},
   "source": [
    "# Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ebddf-09ec-4cbe-b379-538327a5202e",
   "metadata": {},
   "source": [
    "# Deep Q-Network (dqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45889fe0-5143-4cca-8f1a-642ca89a6f8e",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ff5e5-b9c3-4fca-9491-ed4b22eb72fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
